<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="sizhe-li.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="sizhe-li.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-24T03:17:32+00:00</updated><id>sizhe-li.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Rethinking Robot Modeling and Representations</title><link href="sizhe-li.github.io/blog/2025/jacobian-fields-tutorial/" rel="alternate" type="text/html" title="Rethinking Robot Modeling and Representations"/><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>sizhe-li.github.io/blog/2025/jacobian-fields-tutorial</id><content type="html" xml:base="sizhe-li.github.io/blog/2025/jacobian-fields-tutorial/"><![CDATA[<blockquote><p>Was Du ererbt von Deinen Vätern hast, erwirb es, um es zu besitzen. (Goethe)</p><cite><a class="citation" href="#"></a></cite></blockquote> <h1 id="motivation">Motivation</h1> <p>Conventional robots – rigid links, discrete joints, and precise sensors -— are a historical artifact of control and planning approaches. We want to understand an elusive aspect of robotics: <strong>how can robots with diverse morphologies, mechanisms, and sensors be represented, modeled, and controlled in a general-purpose way?</strong></p> <p>Achieving this vision requires new algorithms that go beyond precise sensor feedback and idealized dynamics. Conventional methods struggle with soft and bio-inspired robots, which lack internal sensors and well-defined states. Further, standard definitions of robot actions, such as end-effector pose changes, may not capture the contact-rich and whole-body motions. Finally, general-purpose robot representations could offer new perspectives on appendages and tool use, as scissors and hammers function without embedded sensors but can be considered extensions to the robot dynamics upon contacts.</p> <p>Achieving this vision carries values to our society. Imprecise, flexible, and unconventional robots<d-cite key="gealy2019quasidirectdrivelowcostcompliant"></d-cite> can perform as safely and effectively as traditional designs, paving the way for more affordable, adaptable, and mass-produced robots.</p> <div class="row l-body"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/rigid_motivation.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Conventional robots have limited our algorithmic thinking </figcaption> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/new_robot_motivation.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px"> Next-gen robots require a new algorithmic approach. </figcaption> </div> </div> <h1 id="jacobian-fields">Jacobian Fields</h1> <p>Jacobian Fields is an approach that aims to address problems we posed above. The core idea is to spatialize the conventional end-effector Jacobian in robot dynamics, and compute a field of such operators in the Euclidean space. We encourage the readers to refer to Chapter 5 of Modern Robotics <d-cite key="lynch2017modern"></d-cite> for details on manipulator Jacobians.</p> <h3 id="system-jacobian">System Jacobian</h3> <p>We first derive the conventional system Jacobian<d-cite key="pang2023globalplanningcontactrichmanipulation"></d-cite>. Consider a dynamical system with state $\mathbf{q} \in \mathbb{R}^{m}$, input command $\mathbf{u} \in \mathbb{R}^{n}$, and dynamics $\mathbf{f}: \mathbb{R}^{m} \times \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$. Upon reaching a steady state, the state of the next timestep $\mathbf{q}^{+}$, is given by</p> \[\begin{equation} \mathbf{q}^{+} = \mathbf{f}(\mathbf{q}, \mathbf{u}). \end{equation}\] <p>Local linearization of $\mathbf{f}$ around the nominal point $(\bar{\mathbf{q}}, \bar{\mathbf{u}})$ yields</p> \[\begin{equation} \mathbf{q}^{+} = \mathbf{f}(\bar{\mathbf{q}}, \bar{\mathbf{u}}) + \frac{\partial \mathbf{f}(\mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} \bigg\rvert_{\bar{\mathbf{q}}, \bar{\mathbf{u}}} \delta \mathbf{u}. \end{equation}\] <p>Here, $\mathbf{J}(\mathbf{q}, \mathbf{u}) = \frac{\partial \mathbf{f}(\mathbf{q}, \mathbf{u})}{\partial \mathbf{u}}$ is known as the system Jacobian, the matrix that relates a change of command $\mathbf{u}$ to the change of state $\mathbf{q}$.</p> <p>Conventionally, modeling a robotic system involves experts designing a state vector $\mathbf{q}$ that completely defines the robot state, and then embedding sensors to measure each of these state variables. For example, the piece-wise-rigid morphology of conventional robotic systems means that the set of all joint angles is a full state description, and these are conventionally measured by an angular sensor in each joint.</p> <p>However, these design decisions are difficult to apply to soft robots, and more broadly, to mechanical systems whose coordinates cannot be well-defined or sensorized. Here are a few examples</p> <ul> <li><strong>Soft and Bio-inspired Robots.</strong> Instead of discrete joints, large parts of the robot might deform. Embedding sensors to measure the continuous state of a deformable system is difficult, both because there is no canonical choice for sensors universally compatible with different robots and because their placement and installation are challenging.</li> <li><strong>Designing the state itself is challenging.</strong> In contrast to a piece-wise rigid robot, where the state vector can be a finite-dimensional concatenation of joint angles, the state of a deformable robot is infinite-dimensional due to continuous deformations.</li> <li><strong>Contacts, appendages, wear-and-tear can alter the “coordinate system.”</strong> Grasping on a screwdriver, the Jacobian of the robotic system extends now to newly appended mechanical system. From an engineering standpoint, it is a bit odd to expect that screwdrivers and scissors arrive with sensors reporting their internal states.</li> <li><strong>Multi-sensory integration requires spatialization.</strong> Perceptual signals – visual, tactile, or auditory – reveal the same reality of the physical world precisely in the spatial sense. Traditional expert-designed coordinate systems, hand-crafted on a per-robot basis, do not directly present a general solution to integrating different sensory measurements.</li> </ul> <h3 id="spatialization-of-system-jacobian">Spatialization of system Jacobian</h3> <p>We have argued for the importance of spatialization to overcome modeling challenges. For a solid domain<d-cite key="bonet1997nonlinear"></d-cite>, we can spatialize our differential quantity system Jacobian. This lifts our representation from reduced or minimal coordinates to the Eulidean space and draws deep connections with recent advances in computational perception.</p> <p>To formally describe this idea using continuum mechanics, we consider the spatial dynamics paramterized by a deformation map $\phi(\cdot | \mathbf{q}, \mathbf{u}): \Omega^{0} \mapsto \Omega^{1}$, where $\Omega^{t} \subset \mathbb{R}^{d}$ and $d$ = 2 or 3 is the dimension of our domain. $\mathbf{x} = \phi(\mathbf{X} | \mathbf{q}, \mathbf{u})$ is a flow map that transports the coordinate $\mathbf{X}\in \mathbb{R}^{d}$ in the initial configuration to the configuration achieved at $\mathbf{q}$ and $\mathbf{u}$.</p> <p>Now, similar to the differential relationship above in generalized coordinates, we have the spatial system Jacobian as follows</p> \[\begin{equation} \mathbf{x}^{+} = \mathbf{\phi}(\mathbf{x} | \bar{\mathbf{q}} , \bar{\mathbf{u}}) + \frac{\partial \mathbf{\phi}(\mathbf{x} | \mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} \bigg\rvert_{\bar{\mathbf{q}}, \bar{\mathbf{u}}} \delta \mathbf{u}. \end{equation}\] <blockquote> <p>The spatial differential quantity $\frac{\partial \mathbf{\phi}(\cdot | \mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} $ is precisely the Jacobian Field that we are interested in learning and measuring from perceptual observations.</p> </blockquote> <h3 id="learning-and-measuring-jacobian-fields-from-perceptual-inputs">Learning and measuring Jacobian Fields from perceptual inputs</h3> <p>We now use a simple 2D example to illustrate the idea of Jacobian Fields. Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/tutorial/1_training_pusher_jacobian_in_2D.ipynb">tutorial 1</a> to reproduce our results in this section.</p> <p><strong>2D Pusher Environment.</strong> The environment contains a spherical robotic pusher. The robot can move freely in 2D space and is steered by a 2D velocity command $\delta u \triangleq (x, y)$, where $x, y \in \mathbb{R}$.</p> <p><strong>Training Samples Illustration.</strong> We now illustrate the two training samples in our dataset, moving left and right. The following same-row videos might become async due to html artifact.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_down_rgb.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 1 RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_down_optical_flow.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 1 optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_right_rgb.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 2 RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_right_optical_flow.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 2 optical flow </figcaption> </div> </div> <p>How can we learn Jacobian Fields from visual perception to “explain away” our observed optical flow motions? We first find that our solid domain here, $\Omega^{t}$, equates the pixel space $\mathbb^{H\times W}$ that we perceive. This is not always the case, as the 3D world induces a projection process to imaging devices and contains occluding and refracting surfaces.</p> <p>We use a standard fully convolutional network (UNet<d-cite key="ronneberger2015unetconvolutionalnetworksbiomedical"></d-cite>) to parameterize the inference of Jacobian field from image observations. $J_\theta(\cdot | I(\mathbf{q})) \triangleq \frac{\partial \mathbf{\phi}(\cdot | \mathbf{q})}{\partial \mathbf{u}}$ conditions on an image, and outputs a field of linear operators, i.e., $J_\theta(x | I) \in \mathbb{R}^{N_{u} \times N_{space}}$.</p> <blockquote> <p>One of our core insights is that visual perceptual signals $I(\mathbf{q})$ are functions of latent variable $\mathbf{q}$ that might be hard to directly parameterize, but can be encoded implicitly by a neural network. Indeed, perceptual signals such as images are physical artifacts of lights.</p> </blockquote> <p>Given data pair $(I, I^+, \delta u)$, we can set up the following learning problem</p> <p>\(\begin{equation} \arg\min_{\theta} \ \left\lVert J_\theta \big(x \mid I \big)^\top \delta u - V \big(x \mid I, I^+ \big) \right\rVert, \end{equation}\) where $V$ is the optical flow field computed by a state-of-the-art approach (e.g.,RAFT<d-cite key="teed2020raftrecurrentallpairsfield"></d-cite>).</p> <h3 id="visualizing-the-learned-jacobian-fields">Visualizing the learned Jacobian Fields</h3> <p>Training till convergence, using just two samples, our Jacobian Field is able to explain unseen image observations and command directions. We will soon delve deeper into the physics of why Jacobian Fields can generalize here. The short answers are <strong>invariance, locality, and compositionality</strong> of the Jacobians.</p> <p>How are we visualizing the Jacobians? We simply assign color component to each channel of the Jacobian field. For every spatial coordinate, we compute the norm over spatial channel to derive the total spatial sensitivity vector of the Jacobian. Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field">tutorial 1</a> to reproduce our results in this section.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/rgb_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/gt_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> G.T. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Pred. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_jacobian_q0_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 1 (dx) </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_jacobian_q1_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 2 (dy) </figcaption> </div> </div> <h3 id="a-second-example-in-2d">A second example in 2D.</h3> <p>Let’s train our Jacobian Fields in another environment, but this time more compositional and dexterous! Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/tutorial/2_training_finger_jacobian_in_2D.ipynb">tutorial 2</a> to reproduce our results in this section.</p> <p><strong>Finger Environment.</strong> The environment contains a 2 degrees-of-freedom robot finger. The robot finger is commanded by a 2D joint velocity command $\delta u \triangleq (u_1, u_2)$, where $u_1, u_2 \in \mathbb{R}$ control the rotations of each motor respectively.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/rgb_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/gt_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> G.T. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Pred. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_jacobian_q0_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 1 </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_jacobian_q1_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 2 </figcaption> </div> </div> <p>Fantastic! The same idea can spatially disentangle the different components of motions.</p> <h3 id="unified-representation-with-environment-variables">Unified representation with environment variables</h3> <p>The idea of Jacobian Fields extends to capture dynamics with environment variables, such as contacts with continuum bodies.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/two_finger/control_example.mp4" type="video/mp4"/> </video> <figcaption style="margin-top: 8px; font-style: italic; text-align: center;"> Control Illustration </figcaption> </figure> <h3 id="real-world-examples">Real-world Examples.</h3> <p>Amazingly, the same exact idea extends to the real-world and to 3D. By incorporating the image formation model and neural rendering, we can use optical flows observed with 2D cameras to explain away the underlying flow field in the 3D world.</p> <p>Here are a few examples, please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/real_world/1_visualize_jacobian_fields.ipynb">tutorial 3</a> and <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/real_world/2_inverse_dynamics.ipynb">tutorial 4</a> to reproduce our results in this section.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/njf_allegro.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> <strong> Results on Allegro Hand.</strong> (Left) Input video, (Middle) Depth Prediction, (Right) Jacobian Prediction </figcaption> </div> </div> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/njf_pneumatic.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> <strong>Results on Pneumatic Hand.</strong> (Left) Input Image, (Middle) Depth Prediction, (Right) Jacobian Prediction </figcaption> </div> </div> <h3 id="controlling-the-robot-with-jacobian-fields">Controlling the robot with Jacobian Fields.</h3> <p>After training, Jacobian fields can be used for inverse dynamics. We have new upcoming results showing that a flow planner can be flexibly integrated into the approach, stay tuned!</p> <p>For now, let’s illustrate the idea by just using a keypoint trajectory and a model-predictive controller.</p> <div class="row"> <div class="col-sm"> <img src="/assets/video/njf/inverse_dynamics/desired_motion.png" class="img-fluid rounded z-depth-1" alt="Results on Pneumatic Hand"/> <figcaption style="text-align: center; margin-top: 10px;"> Desired Motions </figcaption> </div> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/inverse_dynamics/optim_process.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Optimization </figcaption> </div> <div class="col-sm"> <img src="/assets/video/njf/inverse_dynamics/output_command.png" style="height: 162px; object-fit: cover;" class="img-fluid rounded z-depth-1" alt="Results on Pneumatic Hand"/> <figcaption style="text-align: center; margin-top: 10px;"> Output Command </figcaption> </div> </div> <h1 id="connections-with-invariance-locality-and-compositionality">Connections with invariance, locality, and compositionality</h1> <h3 id="1-linearity-from-local-theory-of-smoothing">1. Linearity from Local Theory of Smoothing<d-cite key="pang2023globalplanningcontactrichmanipulation"></d-cite></h3> <p>We model <em>differential</em> kinematics by linearizing the system dynamics, which represents 3D motion fields induced by small control commands $\delta u$. In this regime, it is well-known that the 3D motion $\delta x$ of robot 3D points can be described by the space Jacobian and is thus a linear function of control commands, i.e. $\alpha \delta x = \frac{\partial f}{\partial u} (\alpha \delta u)$.</p> <p>This is powerful, as completely specifying the system dynamics for a particular configuration $u’$ requires only $n \times 3$ linearly independent observations of pairs of control commands and induced scene flow, as this fully constrains the space Jacobian for a given configuration for a system with $n$ control channels.</p> <p>For instance, one need not observe the motions for <em>both</em> $-\delta u$ and $\delta u$; it suffices to observe <em>one</em> of them. Similarly, one need not observe $\delta u$ and a scalar multiple $\alpha \delta u$; again, one of them in the training set suffices. This is in stark contrast to parameterizing $f$ as a neural network that directly predicts scene flow given an image and a robot command since the neural network <em>does not</em> model these symmetries and will thus require orders of magnitude more motion observations to adequately model the system dynamics.</p> <h3 id="2-spatial-locality-of-mechanical-systems">2. Spatial Locality of Mechanical Systems</h3> <p>Robot commands often result in highly localized spatial motion. Consider the two finger example we experiment with above, within a “rigid” segment, the jacobian tensors are locally smooth as we spatially perturbe the coordinate value.</p> <h3 id="3-spatial-compositionality-of-mechanical-systems">3. Spatial Compositionality of Mechanical Systems</h3> <p>Robot commands often result in spatial motions composed by the influences of individual command channels. The two linkage robot finger we experiment with above is a great example. For a point inside the second segment, its motion is the integral over individual command channels in the Jacobian tensor field. Indeed, our predicted 2D motion is the summation over two Jacobian channels.</p> <h3 id="project-website">Project Website</h3> <p>For more information about our project, please check out our <a href="https://sizhe-li.github.io/publication/neural_jacobian_field/">project website</a>. Feel free to email <a href="">sizheli@mit.edu</a> or create an issue on our github repo for any questions!</p> <h3 id="acknowledgement">Acknowledgement</h3> <p>Sizhe Lester Li is presenting on behalf of the team in the 2024 paper “Unifying 3D Representation and Control of Diverse Robots with a Single Camera.” We would like to thank Isabella Yu for the visualizations of two finger jacobian fields.</p> <h3 id="bibtex">Bibtex</h3> <p>If you find this blog helpful, please consider citing our work</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{li2024unifying3drepresentationcontrol,
      title={Unifying 3D Representation and Control of Diverse Robots with a Single Camera}, 
      author={Sizhe Lester Li and Annan Zhang and Boyuan Chen and Hanna Matusik and Chao Liu and Daniela Rus and Vincent Sitzmann},
      year={2024},
      eprint={2407.08722},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.08722}, 
}
</code></pre></div></div>]]></content><author><name>Sizhe Lester Li</name></author><summary type="html"><![CDATA[An overview of Neural Jacobian Field, an architecture that autonomously learns to represent, model, and control robots in a general-purpose way.]]></summary></entry></feed>