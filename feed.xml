<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="sizhe-li.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="sizhe-li.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-27T01:20:00+00:00</updated><id>sizhe-li.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Rethinking Robot Modeling and Representations</title><link href="sizhe-li.github.io/blog/2025/jacobian-fields-tutorial/" rel="alternate" type="text/html" title="Rethinking Robot Modeling and Representations"/><published>2025-03-15T00:00:00+00:00</published><updated>2025-03-15T00:00:00+00:00</updated><id>sizhe-li.github.io/blog/2025/jacobian-fields-tutorial</id><content type="html" xml:base="sizhe-li.github.io/blog/2025/jacobian-fields-tutorial/"><![CDATA[<blockquote><p>Was Du ererbt von Deinen Vätern hast, erwirb es, um es zu besitzen. (Goethe)</p><cite><a class="citation" href="#"></a></cite></blockquote> <h1 id="tldr">TL;DR</h1> <p>We present a solution that learns a controllable 3D model of any robot, from vision alone. This includes robots that were previously intractable to model by experts. <a href="https://github.com/sizhe-li/neural-jacobian-field">[Code]</a> <a href="https://sizhe-li.github.io/publication/neural_jacobian_field/">[Project Page]</a> <a href="https://www.youtube.com/watch?v=dFZ1RvJMN7A">[Video]</a></p> <h1 id="motivation">Motivation</h1> <p>Have you ever wondered why robots today are so costly? Why are they almost always made of rigidly jointed segments? What if I tell you that this is largely due to challenges in building digital models of these robots, rather than hardware challenges?</p> <div class="row l-body"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/allegro_cost.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Allegro Hand </figcaption> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/ur5_cost.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> UR5 Arm </figcaption> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/shadow_hand_cost.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px"> Shadow Hand </figcaption> </div> </div> <h3 id="what-even-is-a-robot-anyways">What even is a robot anyways?</h3> <p>Let’s take a step back and think about what a robot even is. Slapping a few motors on an IKEA Lamp with a Raspberry Pi, we have mechanical system that we can command to move around in space. Is that a robot? <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/robotics/history.html">(This turns out to be a rather difficult question)</a>. We would argue that if a machine is capable of solving the physical task you are interested in, then it is a valid robot.</p> <div class="row l-body"> <div class="col-sm text-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/ikea_robot.png" style="width: 50%; display: block; margin: 0 auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> <strong><a href="https://community.robotshop.com/forum/t/tertiarm-low-cost-3d-printed-robot-arm-based-on-ikea-lamp/7517">Tertiarm</a></strong>: Low cost, 3D printed, made of IKEA Lamp </figcaption> </div> </div> <h3 id="the-minimum-requirement--the-ability-to-control">The minimum requirement – the ability to control</h3> <p>Perhaps a valid definition of a robot includes a crtierion of <em>controllability</em>. We need to be able to control the robot to call it one. That’s why many people wouldn’t call the motorized IKEA lamp a robot. Because today we do not have an algorithm that can control it effectively. Next, we have a second robot – a pneumatic hand (<strong>left figure</strong>) – that shares the same property as our IKEA robot. Namely, today this robot is also not controllable.</p> <p>The question of control is – in order for my robot hand to create desired motions what change of command should I send? (<strong>right figure</strong>)</p> <div class="row l-body"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/control_1.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> $300 pneuamtic hand at work </figcaption> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/control_2.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px"> <strong>Desired motions</strong>: arrows; <strong>Commands</strong>: blue air tubes </figcaption> </div> </div> <p>Today, most robots look like the UR5 below. These robots inherit a special class of sensors and morphologies for making them controllable.</p> <div class="row l-body"> <div class="col-sm text-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/rigid_motivation.png" style="width: 85%; display: block; margin: 0 auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Conventional robots might have limited our algorithmic thinking </figcaption> </div> </div> <h3 id="why-is-the-past-solution-no-longer-adequate">Why is the past solution no longer adequate</h3> <p>The way we have approached this problem in the past has reached a ceiling. It turns out the past solutions cannot model the robot above. Since these robots lack sensory measurements and experience deformations.</p> <p>This is a part of why robotic automation today remains costly. We have cheaper hardware designs that can still perform the same physical tasks, but the conventional software does not support these hardwares. As a result, our robots are often over-engineered to compensate the software limitations.</p> <p>For mechanical systems like airplanes and trains, it makes sense to design special-purpose sensors that measure the almost exact states. These sensory measurements typically support a model that suits the problem domains. This is because the application domain for these control problem requires extreme modeling accuracy and reaction times.</p> <p>However, <em>the conventional modeling spirit doesn’t seem suitable for the coming age of general-purpose robotics</em>. It assumes that the environment and the robot are modeled separately. It assumes that scissors and screwdrivers arrive with internal sensors reporting their states. It assumes that robots and the world have a fixed form of morphology and physical properties.</p> <h3 id="a-new-spirit-forward">A new spirit forward</h3> <p>A new modeling spirit is needed for more accessible robotic automation. One that integrates recent advances in computational perception, with powerful tools from machine learning, mechanics, and control. One that lifts the modeling assumptions to a level suitable for general-purpose robotics. One that requires new algorithms that go beyond precise sensor feedback and idealized dynamics.</p> <p>Achieving this vision carries values to our society. Imprecise, flexible, and unconventional robots<d-cite key="gealy2019quasidirectdrivelowcostcompliant"></d-cite> can perform as safely and effectively as traditional designs, paving the way for more affordable, adaptable, and mass-produced robots.</p> <div class="row l-body"> <div class="col-sm text-center"> <img class="img-fluid rounded z-depth-1" src="/assets/img/njf/new_robot_motivation.png" style="width: 85%; display: block; margin: 0 auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Next-gen robots require a new algorithmic approach. </figcaption> </div> </div> <h1 id="jacobian-fields">Jacobian Fields</h1> <p>Jacobian Fields is an approach that aims to address problems we posed above. The core idea is to spatialize the conventional end-effector Jacobian in robot dynamics, and compute a field of such operators in the Euclidean space. We encourage the readers to refer to Chapter 5 of Modern Robotics <d-cite key="lynch2017modern"></d-cite> for details on manipulator Jacobians.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/img/publication_preview/jacobian_fields_teaser.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> <strong> Teaser Video </strong> </figcaption> </div> </div> <h3 id="the-body-jacobian">The Body Jacobian</h3> <p>Look at this 2D robot arm! For any spatial point $(x, y)$ on the robot. Given a small variation of the joint angles $\delta q$, we can find Jacobian of the following form</p> \[\begin{bmatrix} \delta {x} \\ \delta {y} \end{bmatrix} = J(q) \begin{bmatrix} \delta {q}_1 \\ \delta {q}_2 \end{bmatrix}.\] <p>For more formalism and details, please check out <a href="#details-on-system-jacobian">our appendix</a> on the system Jacobian.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/two_finger/only_joint_1_moving.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Only joint 1 moving </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/two_finger/only_joint_2_moving.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Only joint 2 moving </figcaption> </div> </div> <h3 id="spatialization-of-jacobian">Spatialization of Jacobian</h3> <p>We can spatialize our Jacobian. This lifts our representation from reduced or minimal coordinates to the Eulidean space and draws deep connections with recent advances in computational perception.</p> <p>To formally describe this idea using continuum mechanics<d-cite key="bonet1997nonlinear"></d-cite>, we consider the the deformation map $\phi(\cdot | \mathbf{q}, \mathbf{u}): \Omega^{0} \mapsto \Omega^{1}$, where $\Omega^{t} \subset \mathbb{R}^{d}$. $d$ = 2 or 3 depending on our modeling domain. $\mathbf{x} = \phi(\mathbf{X} | \mathbf{q}, \mathbf{u})$ is a flow map that transports the coordinate $\mathbf{X}\in \mathbb{R}^{d}$ in the initial configuration to the configuration achieved at $\mathbf{q}$ and $\mathbf{u}$.</p> <p>Now, we have the spatial system Jacobian as follows</p> \[\begin{equation} \mathbf{x}^{+} = \mathbf{\phi}(\mathbf{x} | \bar{\mathbf{q}} , \bar{\mathbf{u}}) + \frac{\partial \mathbf{\phi}(\mathbf{x} | \mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} \bigg\rvert_{\bar{\mathbf{q}}, \bar{\mathbf{u}}} \delta \mathbf{u}. \end{equation}\] <blockquote> <p>The spatial differential quantity $\frac{\partial \mathbf{\phi}(\cdot | \mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} $ is precisely the Jacobian Field that we are interested in learning and measuring from perceptual observations.</p> </blockquote> <h3 id="learning-and-measuring-jacobian-fields-from-perceptual-inputs">Learning and measuring Jacobian Fields from perceptual inputs</h3> <p>We now use a simple 2D example to illustrate the idea of Jacobian Fields. Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/tutorial/1_training_pusher_jacobian_in_2D.ipynb">tutorial 1</a> to reproduce our results in this section.</p> <p><strong>2D Pusher Environment.</strong> The environment contains a spherical robotic pusher. The robot can move freely in 2D space and is steered by a 2D velocity command $\delta u \triangleq (x, y)$, where $x, y \in \mathbb{R}$.</p> <p><strong>Training Samples Illustration.</strong> We now illustrate the two training samples in our dataset, moving left and right. The following same-row videos might become async due to html artifact.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_down_rgb.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 1 RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_down_optical_flow.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 1 optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_right_rgb.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 2 RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/move_right_optical_flow.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Sample 2 optical flow </figcaption> </div> </div> <p>How can we learn Jacobian Fields from visual perception to “explain away” our observed optical flow motions? We first find that our solid domain here, $\Omega^{t}$, equates the pixel space $\mathbb^{H\times W}$ that we perceive. This is not always the case, as the 3D world induces a projection process to imaging devices and contains occluding and refracting surfaces.</p> <p>We use a standard fully convolutional network (UNet<d-cite key="ronneberger2015unetconvolutionalnetworksbiomedical"></d-cite>) to parameterize the inference of Jacobian field from image observations. $J_\theta(\cdot | I(\mathbf{q})) \triangleq \frac{\partial \mathbf{\phi}(\cdot | \mathbf{q})}{\partial \mathbf{u}}$ conditions on an image, and outputs a field of linear operators.</p> <p>Given data pair $(I, I^+, \delta u)$, we can set up the following learning problem</p> <p>\(\begin{equation} \arg\min_{\theta} \ \left\lVert J_\theta \big(x \mid I \big)^\top \delta u - V \big(x \mid I, I^+ \big) \right\rVert, \end{equation}\) where $V$ is the optical flow field computed by a state-of-the-art approach (e.g.,RAFT<d-cite key="teed2020raftrecurrentallpairsfield"></d-cite>).</p> <h3 id="visualizing-the-learned-jacobian-fields">Visualizing the learned Jacobian Fields</h3> <p>Training till convergence, using just two samples, our Jacobian Field is able to explain unseen image observations and command directions. We will soon delve deeper into the physics of why Jacobian Fields can generalize here. The short answers are <strong>invariance, locality, and compositionality</strong> of the Jacobians.</p> <p>How are we visualizing the Jacobians? We simply assign color component to each channel of the Jacobian field. For every spatial coordinate, we compute the norm over spatial channel to derive the total spatial sensitivity vector of the Jacobian. Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/tutorial/1_training_pusher_jacobian_in_2D.ipynb">tutorial 1</a> to reproduce our results in this section.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/rgb_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/gt_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> G.T. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Pred. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_jacobian_q0_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 1 (dx) </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial1/pred_jacobian_q1_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 2 (dy) </figcaption> </div> </div> <h3 id="a-second-example-in-2d">A second example in 2D.</h3> <p>Let’s train our Jacobian Fields in another environment, but this time more compositional and dexterous! Please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/tutorial/2_training_finger_jacobian_in_2D.ipynb">tutorial 2</a> to reproduce our results in this section.</p> <p><strong>Finger Environment.</strong> The environment contains a 2 degrees-of-freedom robot finger. The robot finger is commanded by a 2D joint velocity command $\delta u \triangleq (u_1, u_2)$, where $u_1, u_2 \in \mathbb{R}$ control the rotations of each motor respectively.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/rgb_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> RGB observation </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/gt_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> G.T. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_optical_flow_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Pred. optical flow </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_jacobian_q0_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 1 </figcaption> </div> <div class="col-sm"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/tutorial2/pred_jacobian_q1_1.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Jac. channel 2 </figcaption> </div> </div> <p>Fantastic! The same idea can spatially disentangle the different components of motions.</p> <h3 id="unified-representation-with-environment-variables">Unified representation with environment variables</h3> <p>The idea of Jacobian Fields extends to capture dynamics with environment variables, such as contacts with continuum bodies.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <video controls="" class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/two_finger/control_example.mp4" type="video/mp4"/> </video> <figcaption style="margin-top: 8px; font-style: italic; text-align: center;"> Control Illustration </figcaption> </figure> <h3 id="real-world-examples">Real-world Examples.</h3> <p>Amazingly, the same exact idea extends to the real-world and to 3D. By incorporating the image formation model and neural rendering, we can use optical flows observed with 2D cameras to explain away the underlying flow field in the 3D world.</p> <p>Here are a few examples, please check out the pytorch implementation of <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/real_world/1_visualize_jacobian_fields.ipynb">tutorial 3</a> and <a href="https://github.com/sizhe-li/neural-jacobian-field/blob/main/notebooks/real_world/2_inverse_dynamics.ipynb">tutorial 4</a> to reproduce our results in this section.</p> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/njf_allegro.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> <strong> Results on Allegro Hand.</strong> (Left) Input video, (Middle) Depth Prediction, (Right) Jacobian Prediction </figcaption> </div> </div> <div class="row"> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/njf_pneumatic.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> <strong>Results on Pneumatic Hand.</strong> (Left) Input Image, (Middle) Depth Prediction, (Right) Jacobian Prediction </figcaption> </div> </div> <h3 id="controlling-the-robot-with-jacobian-fields">Controlling the robot with Jacobian Fields.</h3> <p>After training, Jacobian fields can be used for inverse dynamics. We have new upcoming results showing that a flow planner can be flexibly integrated into the approach, stay tuned!</p> <p>For now, let’s illustrate the idea by just using a keypoint trajectory and a model-predictive controller.</p> <div class="row"> <div class="col-sm"> <img src="/assets/video/njf/inverse_dynamics/desired_motion.png" class="img-fluid rounded z-depth-1" alt="Results on Pneumatic Hand"/> <figcaption style="text-align: center; margin-top: 10px;"> Desired Motions </figcaption> </div> <div class="col-sm"> <video class="img-fluid rounded z-depth-1" autoplay="" loop="" muted="" playsinline=""> <source src="/assets/video/njf/inverse_dynamics/optim_process.mp4" type="video/mp4"/> </video> <figcaption style="text-align: center; margin-top: 10px;"> Optimization </figcaption> </div> <div class="col-sm"> <img src="/assets/video/njf/inverse_dynamics/output_command.png" style="height: 162px; object-fit: cover;" class="img-fluid rounded z-depth-1" alt="Results on Pneumatic Hand"/> <figcaption style="text-align: center; margin-top: 10px;"> Output Command </figcaption> </div> </div> <h1 id="connections-with-invariance-locality-and-compositionality">Connections with invariance, locality, and compositionality</h1> <h3 id="1-linearity-from-local-theory-of-smoothing">1. Linearity from Local Theory of Smoothing<d-cite key="pang2023globalplanningcontactrichmanipulation"></d-cite></h3> <p>We model <em>differential</em> kinematics by linearizing the system dynamics, which represents 3D motion fields induced by small control commands $\delta u$. In this regime, it is well-known that the 3D motion $\delta x$ of robot 3D points can be described by the space Jacobian and is thus a linear function of control commands, i.e. $\alpha \delta x = \frac{\partial f}{\partial u} (\alpha \delta u)$.</p> <p>This is powerful, as completely specifying the system dynamics for a particular configuration $u’$ requires only $n \times 3$ linearly independent observations of pairs of control commands and induced scene flow, as this fully constrains the space Jacobian for a given configuration for a system with $n$ control channels.</p> <p>For instance, one need not observe the motions for <em>both</em> $-\delta u$ and $\delta u$; it suffices to observe <em>one</em> of them. Similarly, one need not observe $\delta u$ and a scalar multiple $\alpha \delta u$; again, one of them in the training set suffices. This is in stark contrast to parameterizing $f$ as a neural network that directly predicts scene flow given an image and a robot command since the neural network <em>does not</em> model these symmetries and will thus require orders of magnitude more motion observations to adequately model the system dynamics.</p> <h3 id="2-spatial-locality-of-mechanical-systems">2. Spatial Locality of Mechanical Systems</h3> <p>Robot commands often result in highly localized spatial motion. Consider the two finger example we experiment with above, within a “rigid” segment, the jacobian tensors are locally smooth as we spatially perturbe the coordinate value.</p> <h3 id="3-spatial-compositionality-of-mechanical-systems">3. Spatial Compositionality of Mechanical Systems</h3> <p>Robot commands often result in spatial motions composed by the influences of individual command channels. The two linkage robot finger we experiment with above is a great example. For a point inside the second segment, its motion is the integral over individual command channels in the Jacobian tensor field. Indeed, our predicted 2D motion is the summation over two Jacobian channels.</p> <h3 id="project-website">Project Website</h3> <p>For more information about our project, please check out our <a href="https://sizhe-li.github.io/publication/neural_jacobian_field/">project website</a>. Feel free to email <a href="">sizheli@mit.edu</a> or create an issue on our github repo for any questions!</p> <h3 id="related-works">Related Works</h3> <p>(to be finished, Lester)</p> <h3 id="acknowledgement">Acknowledgement</h3> <p>Sizhe Lester Li is presenting on behalf of the team in the 2024 paper “Unifying 3D Representation and Control of Diverse Robots with a Single Camera.” We would like to thank Isabella Yu for the visualizations of two finger jacobian fields.</p> <h3 id="bibtex">Bibtex</h3> <p>If you find this blog helpful, please consider citing our work</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{li2024unifying3drepresentationcontrol,
      title={Unifying 3D Representation and Control of Diverse Robots with a Single Camera}, 
      author={Sizhe Lester Li and Annan Zhang and Boyuan Chen and Hanna Matusik and Chao Liu and Daniela Rus and Vincent Sitzmann},
      year={2024},
      eprint={2407.08722},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.08722}, 
}
</code></pre></div></div> <h3 id="details-on-system-jacobian">Details on System Jacobian</h3> <p>We first derive the conventional system Jacobian<d-cite key="pang2023globalplanningcontactrichmanipulation"></d-cite>. Consider a dynamical system with state $\mathbf{q} \in \mathbb{R}^{m}$, input command $\mathbf{u} \in \mathbb{R}^{n}$, and dynamics $\mathbf{f}: \mathbb{R}^{m} \times \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$. Upon reaching a steady state, the state of the next timestep $\mathbf{q}^{+}$, is given by</p> \[\begin{equation} \mathbf{q}^{+} = \mathbf{f}(\mathbf{q}, \mathbf{u}). \end{equation}\] <p>Local linearization of $\mathbf{f}$ around the nominal point $(\bar{\mathbf{q}}, \bar{\mathbf{u}})$ yields</p> \[\begin{equation} \mathbf{q}^{+} = \mathbf{f}(\bar{\mathbf{q}}, \bar{\mathbf{u}}) + \frac{\partial \mathbf{f}(\mathbf{q}, \mathbf{u})}{\partial \mathbf{u}} \bigg\rvert_{\bar{\mathbf{q}}, \bar{\mathbf{u}}} \delta \mathbf{u}. \end{equation}\] <p>Here, $\mathbf{J}(\mathbf{q}, \mathbf{u}) = \frac{\partial \mathbf{f}(\mathbf{q}, \mathbf{u})}{\partial \mathbf{u}}$ is known as the system Jacobian, the matrix that relates a change of command $\mathbf{u}$ to the change of state $\mathbf{q}$.</p> <p>Conventionally, modeling a robotic system involves experts designing a state vector $\mathbf{q}$ that completely defines the robot state, and then embedding sensors to measure each of these state variables. For example, the piece-wise-rigid morphology of conventional robotic systems means that the set of all joint angles is a full state description, and these are conventionally measured by an angular sensor in each joint.</p> <p>However, these design decisions are difficult to apply to soft robots, and more broadly, to mechanical systems whose coordinates cannot be well-defined or sensorized. Here are a few examples</p> <ul> <li><strong>Soft and Bio-inspired Robots.</strong> Instead of discrete joints, large parts of the robot might deform. Embedding sensors to measure the continuous state of a deformable system is difficult, both because there is no canonical choice for sensors universally compatible with different robots and because their placement and installation are challenging.</li> <li><strong>Designing the state itself is challenging.</strong> In contrast to a piece-wise rigid robot, where the state vector can be a finite-dimensional concatenation of joint angles, the state of a deformable robot is infinite-dimensional due to continuous deformations.</li> <li><strong>Contacts, appendages, wear-and-tear can alter the “coordinate system.”</strong> Grasping on a screwdriver, the notion of the robotic system extends to the appendage. From an engineering standpoint, it is a bit odd to expect that screwdrivers and scissors arrive with sensors reporting their internal states.</li> <li><strong>Multi-sensory integration requires spatialization.</strong> Perceptual signals – visual, tactile, or auditory – reveal the same reality of the physical world precisely in the spatial sense. Traditional expert-designed coordinate systems, hand-crafted on a per-robot basis, do not directly present a general solution to integrating different sensory measurements.</li> </ul>]]></content><author><name>Sizhe Lester Li</name></author><summary type="html"><![CDATA[An overview of Neural Jacobian Field, an architecture that autonomously learns to represent, model, and control robots in a general-purpose way.]]></summary></entry></feed>